# First-Visit Monte Carlo Prediction
This project implements the First-Visit Monte Carlo (MC) Prediction Algorithm to estimate the state-value function for a Blackjack environment. The goal is to learn the expected reward of each state under a given policy by simulating multiple episodes and averaging the observed returns.
Monte Carlo methods are particularly useful for model-free reinforcement learning, where the transition probabilities and rewards of the environment are unknown. By applying First-Visit Monte Carlo, the algorithm learns from experience, making it ideal for games like Blackjack, where outcomes are stochastic and rewards depend on sequences of actions.
